{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vrH2dezK2QF"
      },
      "source": [
        "# RNN (Sentimental Analysis for whatsapp Chat)\n",
        "## (Roman Urdu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkqk99WvK2QN"
      },
      "source": [
        "### Import all required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xnUnh6G1K2QR"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BwknNujK2Qa"
      },
      "source": [
        "## Read Dataset and Show all Unique Class count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1rTiYaUXK2Qb",
        "outputId": "393d225a-4445-4276-beb8-0e11c9fdcf9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Neutral     8929\n",
              "Positive    6013\n",
              "Negative    5287\n",
              "Name: Mode, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "ROOT_PATH = '/content'\n",
        "dataset = pd.read_csv(ROOT_PATH + '/Whatsapp_chat.csv', usecols=[0,1])\n",
        "#dataset = dataset[dataset['Mode'].isin(['Positive', 'Negative'])]\n",
        "dataset[\"Mode\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rLssZU7RK2Qd"
      },
      "outputs": [],
      "source": [
        "data_pos = dataset[dataset['Mode'] == 'Positive'].iloc[:5000]\n",
        "data_neg = dataset[dataset['Mode'] == 'Negative'].iloc[:5000]\n",
        "data_neu = dataset[dataset['Mode'] == 'Neutral'].iloc[:5000]\n",
        "\n",
        "dataset = pd.concat([data_pos,data_neg,data_neu])\n",
        "\n",
        "dataset = dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IPfvXmSMK2Qe",
        "outputId": "0e4f0685-dacb-42ad-de2b-8fc946f61c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive    0.333333\n",
            "Negative    0.333333\n",
            "Neutral     0.333333\n",
            "Name: Mode, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(dataset['Mode'].value_counts(normalize=True))\n",
        "baseline = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vJzWqruqK2Qf"
      },
      "outputs": [],
      "source": [
        "## Cleaning Process\n",
        "data=[]\n",
        "stopwords=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh', 'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya', 'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se', 'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski', 'ne', 'haan', 'nai', 'sent', 'aj', 'you', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya', 'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi', 'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain', 'krny', 'tou']\n",
        "for i in range(dataset.shape[0]):\n",
        "    message = (re.sub('[^a-zA-Z]',' ',str(dataset.iloc[:,0].values[i]))).lower().split()\n",
        "    message = [word for word in message if not word in stopwords]\n",
        "    message = ' '.join(message)\n",
        "    data.append(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LBNE0G63K2Qg",
        "outputId": "2ce46cf3-2a37-488e-dbd3-7afd658d474a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens in Data:  27471\n"
          ]
        }
      ],
      "source": [
        "# Total number of word is 31466 in my corpus\n",
        "## The parameter in Keras Tokenizer \"number_words\" return the ids of the most 5000 frequent words\n",
        "## By default the tokenizer split on base space \" \"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, split=\" \")\n",
        "tokenizer.fit_on_texts(data)\n",
        "X = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "print(\"Total Tokens in Data: \",len(tokenizer.word_index))\n",
        "\n",
        "X = pad_sequences(X) # padding our text vector so they all have the same length (maximum lenth sub_list in list array)\n",
        "\n",
        "# maxList = max(X, key = lambda i: len(i))\n",
        "# maxLength = len(maxList)\n",
        "maxLength = len(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CSw8c7lKK2Qi"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(5000, 256, input_length=X.shape[1]))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\n",
        "model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0VQUoCZpK2Qj",
        "outputId": "b184b97e-d615-4621-c8c6-45390153533c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 188, 256)          1280000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 188, 256)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 188, 256)          525312    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2331395 (8.89 MB)\n",
            "Trainable params: 2331395 (8.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cGgFAYszK2Ql",
        "outputId": "2e7586a0-0e75-42ed-c2cf-2fb9cb4b1917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral [0 1 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "y = pd.get_dummies(dataset['Mode']).values\n",
        "\n",
        "[print(dataset['Mode'][i], y[i]) for i in range(12000,12001)]\n",
        "\n",
        "# Positive [0 0 1]\n",
        "# Neutral [0 1 0]\n",
        "# Negative [1 0 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YVGhvKKLK2Qm"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kgcv7L6PK2Qn",
        "outputId": "a834f97f-61fb-4d9f-94d6-60cddc828106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "42/42 [==============================] - 266s 6s/step - loss: 1.0515 - accuracy: 0.4498\n",
            "Epoch 2/8\n",
            "42/42 [==============================] - 259s 6s/step - loss: 0.8285 - accuracy: 0.6365\n",
            "Epoch 3/8\n",
            "42/42 [==============================] - 259s 6s/step - loss: 0.6533 - accuracy: 0.7253\n",
            "Epoch 4/8\n",
            "42/42 [==============================] - 260s 6s/step - loss: 0.5399 - accuracy: 0.7815\n",
            "Epoch 5/8\n",
            "42/42 [==============================] - 263s 6s/step - loss: 0.4781 - accuracy: 0.8044\n",
            "Epoch 6/8\n",
            "42/42 [==============================] - 259s 6s/step - loss: 0.4133 - accuracy: 0.8372\n",
            "Epoch 7/8\n",
            "42/42 [==============================] - 259s 6s/step - loss: 0.3964 - accuracy: 0.8436\n",
            "Epoch 8/8\n",
            "42/42 [==============================] - 259s 6s/step - loss: 0.3353 - accuracy: 0.8690\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "epochs = 8\n",
        "'''\n",
        "By default verbose = 1,\n",
        "\n",
        "verbose = 1, which includes both progress bar and one line per epoch\n",
        "\n",
        "verbose = 0, means silent\n",
        "\n",
        "verbose = 2, one line per epoch i.e. epoch no./total no. of epochs\n",
        "'''\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OpeBQKu4K2Qo"
      },
      "outputs": [],
      "source": [
        "model.save('sentiment_analysis.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zgv3lrYZK2Qp",
        "outputId": "85952dc8-0c37-4b3d-8d35-01262db59cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 [==============================] - 47s 334ms/step\n",
            "Wah kya baat likhi [1.2030942e-02 3.2612565e-04 9.8764288e-01] [0 0 1]\n",
            "Wha Itni sari khubiya [0.05525462 0.41765836 0.527087  ] [0 1 0]\n",
            "Itni khubiya [7.2450133e-04 9.9682951e-01 2.4460657e-03] [0 1 0]\n",
            "Ya allah rehm farma hm sab pe or zalimo ko hidayat de ameen [9.9840450e-01 2.7366131e-04 1.3217682e-03] [1 0 0]\n",
            "Please Everyone AllAh S.w.T ka naam hAmesha Bary Lawzo main Likha kary Wo he Zaat sUb say Bari Hey [0.80808234 0.11093297 0.08098475] [0 0 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "[print(dataset['Message'][i], predictions[i], y_test[i]) for i in range(5, 10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UBU3CCY0K2Qq",
        "outputId": "7185a165-b9de-4699-bdaf-c88a4e3ca9ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive predictions: 1534\n",
            "Neutral predictions: 1489\n",
            "Negative predictions: 1477\n",
            "Real positive: 1450\n",
            "Real neutral: 1501\n",
            "Real negative: 1549\n"
          ]
        }
      ],
      "source": [
        "pos_count, neu_count, neg_count = 0, 0, 0\n",
        "real_pos, real_neu, real_neg = 0, 0, 0\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if np.argmax(prediction)==2:\n",
        "        pos_count += 1\n",
        "    elif np.argmax(prediction)==1:\n",
        "        neu_count += 1\n",
        "    else:\n",
        "        neg_count += 1\n",
        "\n",
        "    if np.argmax(y_test[i])==2:\n",
        "        real_pos += 1\n",
        "    elif np.argmax(y_test[i])==1:\n",
        "        real_neu += 1\n",
        "    else:\n",
        "        real_neg +=1\n",
        "\n",
        "print('Positive predictions:', pos_count)\n",
        "print('Neutral predictions:', neu_count)\n",
        "print('Negative predictions:', neg_count)\n",
        "print('Real positive:', real_pos)\n",
        "print('Real neutral:', real_neu)\n",
        "print('Real negative:', real_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiSHbinUK2Qr"
      },
      "outputs": [],
      "source": [
        "# new_model = tf.keras.models.load_model('sentiment_analysis.h5')\n",
        "# predictions = new_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBjEigL-K2Qr",
        "outputId": "b7ae839e-3df7-43c7-ff05-bd549b039a73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor 'strided_slice_2:0' shape=(10,) dtype=int64>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# predicted_categories = tf.argmax(predictions, axis=1)\n",
        "# predicted_categories[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_odHGojWK2Qt",
        "outputId": "539849b6-cfb0-42f3-d9f5-2e1259192504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor 'strided_slice_3:0' shape=(10,) dtype=int64>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# true_categories = tf.argmax(y_test, axis=1)\n",
        "# true_categories[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5LTkFSiK2Qv"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "# import seaborn as sns\n",
        "# from matplotlib import pyplot as plt\n",
        "# sns.set()\n",
        "\n",
        "# #print(y_test)\n",
        "\n",
        "# mat = confusion_matrix(true_categories, predicted_categories)\n",
        "# print(mat)\n",
        "\n",
        "# sns.heatmap(mat.T,square=True,annot=True,fmt='d',cbar=False,\n",
        "#            xticklabels=np.unique(y_test),yticklabels=np.unique(y_test))\n",
        "# plt.xlabel(\"True Label\")\n",
        "# plt.ylabel(\"Predicted Label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQ7nWI-K2Qw"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','test'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train','test'], loc='upper left')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmC0oATYK2Qy",
        "outputId": "ee69fa99-b11e-484b-df94-48170411af7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ],
      "source": [
        "#import tensorflow as tf\n",
        "new_model = load_model('sentiment_analysis.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk7qrEiAK2Q0",
        "outputId": "10ba924c-2df3-4c37-f5ae-c501fe2f1829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.14851262 0.63203645 0.21945095]] Neutral\n",
            "[[0.99647385 0.0018047  0.00172148]] Negative\n",
            "[[0.43794864 0.49018383 0.07186751]] Neutral\n"
          ]
        }
      ],
      "source": [
        "#/* ~~~~~~~~~~~ Model Predict User Input ~~~~~~~~~~~~ */\n",
        "new_text = ['Wah je Waah, kya bat han', 'Lanat hy police walo py ðŸ˜’', 'ye galt bat ha']\n",
        "seq = tokenizer.texts_to_sequences(new_text)\n",
        "padded = pad_sequences(seq, maxlen=maxLength)\n",
        "for case in range(len(seq)):\n",
        "  pred = new_model.predict(padded[[case]])\n",
        "  labels = ['Negative', 'Neutral', 'Positive']\n",
        "  print(pred, labels[np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObP4BPiMK2Q3"
      },
      "outputs": [],
      "source": [
        "def startsWithDateAndTime(s):\n",
        "    pattern = '[0-9]{2}/[0-9]{2}/[0-9]{4}' # for New Group settings\n",
        "    result = re.match(pattern, s)\n",
        "    if result:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5TfmLUFK2Q4"
      },
      "outputs": [],
      "source": [
        "def getDataPoint(line):\n",
        "    splitLine = line.split(' - ')\n",
        "    dateTime = splitLine[0]\n",
        "    date, time = dateTime.split(', ')\n",
        "    message = ' '.join(splitLine[1:])\n",
        "    splitMessage = message.split(': ')\n",
        "    author = splitMessage[0]\n",
        "    message = ' '.join(splitMessage[1:])\n",
        "    return date, time, author, message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STFGqogZK2Q5"
      },
      "outputs": [],
      "source": [
        "parsedData = [] # List to keep track of data so it can be used by a Pandas dataframe\n",
        "### Uploading exported chat file\n",
        "conversationPath = 'chat_with_Shakir.txt' # chat file\n",
        "with open(conversationPath, encoding=\"utf-8\") as fp:\n",
        "    ### Skipping first line of the file because contains information related to something about end-to-end encryption\n",
        "    fp.readline()\n",
        "\n",
        "    messageBuffer = []\n",
        "    date, time, author = None, None, None\n",
        "    while True:\n",
        "        line = fp.readline()\n",
        "        #print(line)\n",
        "        if not line:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        if startsWithDateAndTime(line):\n",
        "            #print('..........')\n",
        "            if len(messageBuffer) > 0:\n",
        "                parsedData.append([date, time, author, ' '.join(messageBuffer)])\n",
        "            messageBuffer.clear()\n",
        "            date, time, author, message = getDataPoint(line)\n",
        "            messageBuffer.append(message)\n",
        "        else:\n",
        "            messageBuffer.append(line)\n",
        "\n",
        "print(len(parsedData))\n",
        "\n",
        "SHChat = pd.DataFrame(parsedData, columns=['Date', 'Time', 'Author', 'Message']) # Initialising a pandas Dataframe.\n",
        "SHChat[\"Date\"] = pd.to_datetime(SHChat[\"Date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28DEHs-_K2Q7"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning process\n",
        "### Counting number of letters in each message\n",
        "SHChat['Words'] = SHChat['Message'].apply(lambda s : len(s.split(' ')))\n",
        "SHChat = SHChat[SHChat['Message'] != '<Media omitted>']\n",
        "SHChat = SHChat[SHChat['Words'] != 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PFBobtQK2Q8"
      },
      "outputs": [],
      "source": [
        "Hassan_Chat = SHChat[SHChat['Author'] == 'HR']\n",
        "Shakir_Chat = SHChat[SHChat['Author'] == 'Shakir MS_27']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5oH0-izK2Q9"
      },
      "outputs": [],
      "source": [
        "Hassan_msg = Hassan_Chat['Message'].to_list()\n",
        "Shakir_msg = Shakir_Chat['Message'].to_list()\n",
        "def pred_arr(msg):\n",
        "  seq = tokenizer.texts_to_sequences(msg)\n",
        "  padded = pad_sequences(seq, maxlen=maxLength)\n",
        "\n",
        "  pos_count, neu_count, neg_count = 0, 0, 0\n",
        "  for case in range(len(seq)):\n",
        "    #print(Hassan_message[case])\n",
        "    pred = model.predict(padded[[case]])\n",
        "    labels = ['Negative', 'Neutral', 'Positive']\n",
        "    #print(pred, labels[np.argmax(pred)])\n",
        "    if np.argmax(pred)==2:\n",
        "        pos_count += 1\n",
        "    elif np.argmax(pred)==1:\n",
        "        neu_count += 1\n",
        "    else:\n",
        "        neg_count += 1\n",
        "\n",
        "  print('Positive predictions:', pos_count)\n",
        "  print('Neutral predictions:', neu_count)\n",
        "  print('Negative predictions:', neg_count)\n",
        "\n",
        "  p = [neg_count, neu_count, pos_count]\n",
        "  return p\n",
        "\n",
        "data1 = np.array(pred_arr(Hassan_msg))\n",
        "data2 = np.array(pred_arr(Shakir_msg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlYT4SGhK2Q-"
      },
      "outputs": [],
      "source": [
        "myexplode = [0.1, 0.1, 0.1]\n",
        "\n",
        "# plt.pie(p, labels = labels, explode = myexplode, shadow = True)\n",
        "# plt.legend(title='Shakir',loc =\"upper left\",)\n",
        "# plt.show()\n",
        "\n",
        "# plt.subplot(121)\n",
        "# plt.pie(data1, labels = labels, explode = myexplode, shadow = True)\n",
        "# plt.subplot(133)\n",
        "# plt.pie(data2, labels = labels, explode = myexplode, shadow = True)\n",
        "# plt.show()\n",
        "\n",
        "# create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "ax1.pie(data1, # Values\n",
        "        labels = labels, # Labels for each sections\n",
        "        explode = myexplode, # To slice the perticuler section\n",
        "        autopct = \"%0.2f%%\", # Show data in persentage for with 2 decimal point\n",
        "        shadow = True, # Showing shadow of pie chart\n",
        "        radius = 1.2, # Radius to increase or decrease the size of pie chart\n",
        "        startangle = 270, # Start angle of first section\n",
        "        )\n",
        "ax2.pie(data2, # Values\n",
        "        labels = labels, # Labels for each sections\n",
        "        explode = myexplode, # To slice the perticuler section\n",
        "        autopct = \"%0.2f%%\", # Show data in persentage for with 2 decimal point\n",
        "        shadow = True, # Showing shadow of pie chart\n",
        "        radius = 1.2, # Radius to increase or decrease the size of pie chart\n",
        "        startangle = 270, # Start angle of first section\n",
        "        )\n",
        "# plot each pie chart in a separate subplot\n",
        "#ax1.pie(data1, labels = labels, explode = myexplode, shadow = True)\n",
        "#ax2.pie(data2, labels = labels, explode = myexplode, shadow = True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG2qyLt-K2Q_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "bd76af128d7c40b48583a811a9c1879190eadb08cc3a308c5c2f1ddbc04e5a8b"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}