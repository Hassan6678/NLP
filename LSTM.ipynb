{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Sentimental Analysis for whatsapp Chat)\n",
    "## (Roman Urdu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset and Show all Unique Class count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     8929\n",
       "Positive    6013\n",
       "Negative    5287\n",
       "Name: Mode, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH = 'C:/Users/Hassan Raza/Desktop/Whatsapp_chat_RNN'\n",
    "dataset = pd.read_csv(ROOT_PATH + '/Whatsapp_chat.csv', usecols=[0,1])\n",
    "#dataset = dataset[dataset['Mode'].isin(['Positive', 'Negative'])]\n",
    "dataset[\"Mode\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = dataset[dataset['Mode'] == 'Positive'].iloc[:5000]\n",
    "data_neg = dataset[dataset['Mode'] == 'Negative'].iloc[:5000]\n",
    "data_neu = dataset[dataset['Mode'] == 'Neutral'].iloc[:5000]\n",
    "\n",
    "dataset = pd.concat([data_pos,data_neg,data_neu])\n",
    "\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral     0.333333\n",
      "Negative    0.333333\n",
      "Positive    0.333333\n",
      "Name: Mode, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Mode'].value_counts(normalize=True))\n",
    "baseline = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Process\n",
    "data=[]\n",
    "stopwords=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh', 'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya', 'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se', 'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski', 'ne', 'haan', 'nai', 'sent', 'aj', 'you', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya', 'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi', 'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain', 'krny', 'tou']\n",
    "for i in range(dataset.shape[0]):\n",
    "    message = (re.sub('[^a-zA-Z]',' ',str(dataset.iloc[:,0].values[i]))).lower().split()\n",
    "    message = [word for word in message if not word in stopwords]\n",
    "    message = ' '.join(message)\n",
    "    data.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens in Data:  27471\n"
     ]
    }
   ],
   "source": [
    "# Total number of word is 31466 in my corpus\n",
    "## The parameter in Keras Tokenizer \"number_words\" return the ids of the most 5000 frequent words\n",
    "## By default the tokenizer split on base space \" \"\n",
    "tokenizer = Tokenizer(num_words=5000, split=\" \")\n",
    "tokenizer.fit_on_texts(data)\n",
    "X = tokenizer.texts_to_sequences(data)\n",
    "print(\"Total Tokens in Data: \",len(tokenizer.word_index))\n",
    "X = pad_sequences(X) # padding our text vector so they all have the same length (maximum lenth sub_list in list array)\n",
    "#print(X[0])\n",
    "\n",
    "# maxList = max(X, key = lambda i: len(i))\n",
    "# maxLength = len(maxList)\n",
    "maxLength = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 256, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 188, 256)          1280000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 188, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 188, 256)          525312    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,331,395\n",
      "Trainable params: 2,331,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral [0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(dataset['Mode']).values\n",
    "\n",
    "[print(dataset['Mode'][i], y[i]) for i in range(12000,12001)]\n",
    "\n",
    "# Positive [0 0 1]\n",
    "# Neutral [0 1 0]\n",
    "# Negative [1 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "10500/10500 [==============================] - 709s 67ms/step - loss: 1.0341 - acc: 0.4635\n",
      "Epoch 2/8\n",
      "10500/10500 [==============================] - 1144s 109ms/step - loss: 0.8137 - acc: 0.6416\n",
      "Epoch 3/8\n",
      "10500/10500 [==============================] - 1443s 137ms/step - loss: 0.6443 - acc: 0.7341\n",
      "Epoch 4/8\n",
      "10500/10500 [==============================] - 1450s 138ms/step - loss: 0.5463 - acc: 0.7796\n",
      "Epoch 5/8\n",
      "10500/10500 [==============================] - 1527s 145ms/step - loss: 0.4882 - acc: 0.8039\n",
      "Epoch 6/8\n",
      "10500/10500 [==============================] - 1627s 155ms/step - loss: 0.4281 - acc: 0.8288\n",
      "Epoch 7/8\n",
      "10500/10500 [==============================] - 1725s 164ms/step - loss: 0.3924 - acc: 0.8483\n",
      "Epoch 8/8\n",
      "10500/10500 [==============================] - 1808s 172ms/step - loss: 0.3591 - acc: 0.8584\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 8\n",
    "'''\n",
    "By default verbose = 1,\n",
    "\n",
    "verbose = 1, which includes both progress bar and one line per epoch\n",
    "\n",
    "verbose = 0, means silent\n",
    "\n",
    "verbose = 2, one line per epoch i.e. epoch no./total no. of epochs\n",
    "'''\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wah kya baat likhi [0.64506716 0.34856185 0.00637098] [0 1 0]\n",
      "Wha Itni sari khubiya [0.60560066 0.37095395 0.0234454 ] [0 1 0]\n",
      "Itni khubiya [0.13057727 0.8137635  0.05565921] [0 1 0]\n",
      "Ya allah rehm farma hm sab pe or zalimo ko hidayat de ameen [0.06729884 0.56694263 0.36575857] [1 0 0]\n",
      "Please Everyone AllAh S.w.T ka naam hAmesha Bary Lawzo main Likha kary Wo he Zaat sUb say Bari Hey [0.31156573 0.01841947 0.6700148 ] [0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "[print(dataset['Message'][i], predictions[i], y_test[i]) for i in range(5, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive predictions: 1120\n",
      "Neutral predictions: 1835\n",
      "Negative predictions: 1091\n",
      "Real positive: 1182\n",
      "Real neutral: 1832\n",
      "Real negative: 1032\n"
     ]
    }
   ],
   "source": [
    "pos_count, neu_count, neg_count = 0, 0, 0\n",
    "real_pos, real_neu, real_neg = 0, 0, 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if np.argmax(prediction)==2:\n",
    "        pos_count += 1\n",
    "    elif np.argmax(prediction)==1:\n",
    "        neu_count += 1\n",
    "    else:\n",
    "        neg_count += 1\n",
    "    \n",
    "    if np.argmax(y_test[i])==2:\n",
    "        real_pos += 1\n",
    "    elif np.argmax(y_test[i])==1:    \n",
    "        real_neu += 1\n",
    "    else:\n",
    "        real_neg +=1\n",
    "\n",
    "print('Positive predictions:', pos_count)\n",
    "print('Neutral predictions:', neu_count)\n",
    "print('Negative predictions:', neg_count)\n",
    "print('Real positive:', real_pos)\n",
    "print('Real neutral:', real_neu)\n",
    "print('Real negative:', real_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = tf.keras.models.load_model('sentiment_analysis.h5')\n",
    "# predictions = new_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_2:0' shape=(10,) dtype=int64>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_categories = tf.argmax(predictions, axis=1)\n",
    "# predicted_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_3:0' shape=(10,) dtype=int64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true_categories = tf.argmax(y_test, axis=1)\n",
    "# true_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# from matplotlib import pyplot as plt\n",
    "# sns.set()\n",
    "\n",
    "# #print(y_test)\n",
    "\n",
    "# mat = confusion_matrix(true_categories, predicted_categories)\n",
    "# print(mat)\n",
    "\n",
    "# sns.heatmap(mat.T,square=True,annot=True,fmt='d',cbar=False,\n",
    "#            xticklabels=np.unique(y_test),yticklabels=np.unique(y_test))\n",
    "# plt.xlabel(\"True Label\")\n",
    "# plt.ylabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train','test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train','test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "new_model = load_model('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14851262 0.63203645 0.21945095]] Neutral\n",
      "[[0.99647385 0.0018047  0.00172148]] Negative\n",
      "[[0.43794864 0.49018383 0.07186751]] Neutral\n"
     ]
    }
   ],
   "source": [
    "#/* ~~~~~~~~~~~ Model Predict User Input ~~~~~~~~~~~~ */\n",
    "new_text = ['Wah je Waah, kya bat han', 'Lanat hy police walo py ðŸ˜’', 'ye galt bat ha']\n",
    "seq = tokenizer.texts_to_sequences(new_text)\n",
    "padded = pad_sequences(seq, maxlen=maxLength)\n",
    "for case in range(len(seq)):  \n",
    "  pred = new_model.predict(padded[[case]])\n",
    "  labels = ['Negative', 'Neutral', 'Positive']\n",
    "  print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithDateAndTime(s):\n",
    "    pattern = '[0-9]{2}/[0-9]{2}/[0-9]{4}' # for New Group settings\n",
    "    result = re.match(pattern, s)\n",
    "    if result:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPoint(line):   \n",
    "    splitLine = line.split(' - ') \n",
    "    dateTime = splitLine[0]\n",
    "    date, time = dateTime.split(', ') \n",
    "    message = ' '.join(splitLine[1:])\n",
    "    splitMessage = message.split(': ') \n",
    "    author = splitMessage[0] \n",
    "    message = ' '.join(splitMessage[1:])\n",
    "    return date, time, author, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = [] # List to keep track of data so it can be used by a Pandas dataframe\n",
    "### Uploading exported chat file\n",
    "conversationPath = 'chat_with_Shakir.txt' # chat file\n",
    "with open(conversationPath, encoding=\"utf-8\") as fp:\n",
    "    ### Skipping first line of the file because contains information related to something about end-to-end encryption\n",
    "    fp.readline()\n",
    "    \n",
    "    messageBuffer = [] \n",
    "    date, time, author = None, None, None\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        #print(line)\n",
    "        if not line: \n",
    "            break\n",
    "        line = line.strip() \n",
    "        if startsWithDateAndTime(line):\n",
    "            #print('..........')\n",
    "            if len(messageBuffer) > 0:\n",
    "                parsedData.append([date, time, author, ' '.join(messageBuffer)]) \n",
    "            messageBuffer.clear() \n",
    "            date, time, author, message = getDataPoint(line) \n",
    "            messageBuffer.append(message) \n",
    "        else:\n",
    "            messageBuffer.append(line)\n",
    "    \n",
    "print(len(parsedData))\n",
    "\n",
    "SHChat = pd.DataFrame(parsedData, columns=['Date', 'Time', 'Author', 'Message']) # Initialising a pandas Dataframe.\n",
    "SHChat[\"Date\"] = pd.to_datetime(SHChat[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning process \n",
    "### Counting number of letters in each message\n",
    "SHChat['Words'] = SHChat['Message'].apply(lambda s : len(s.split(' ')))\n",
    "SHChat = SHChat[SHChat['Message'] != '<Media omitted>']\n",
    "SHChat = SHChat[SHChat['Words'] != 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hassan_Chat = SHChat[SHChat['Author'] == 'HR']\n",
    "Shakir_Chat = SHChat[SHChat['Author'] == 'Shakir MS_27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hassan_msg = Hassan_Chat['Message'].to_list()\n",
    "Shakir_msg = Shakir_Chat['Message'].to_list()\n",
    "def pred_arr(msg):\n",
    "  seq = tokenizer.texts_to_sequences(msg)\n",
    "  padded = pad_sequences(seq, maxlen=maxLength)\n",
    "\n",
    "  pos_count, neu_count, neg_count = 0, 0, 0\n",
    "  for case in range(len(seq)):\n",
    "    #print(Hassan_message[case])\n",
    "    pred = model.predict(padded[[case]])\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    #print(pred, labels[np.argmax(pred)])\n",
    "    if np.argmax(pred)==2:\n",
    "        pos_count += 1\n",
    "    elif np.argmax(pred)==1:\n",
    "        neu_count += 1\n",
    "    else:\n",
    "        neg_count += 1\n",
    "\n",
    "  print('Positive predictions:', pos_count)\n",
    "  print('Neutral predictions:', neu_count)\n",
    "  print('Negative predictions:', neg_count)\n",
    "\n",
    "  p = [neg_count, neu_count, pos_count]\n",
    "  return p\n",
    "\n",
    "data1 = np.array(pred_arr(Hassan_msg))\n",
    "data2 = np.array(pred_arr(Shakir_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myexplode = [0.1, 0.1, 0.1]\n",
    "\n",
    "# plt.pie(p, labels = labels, explode = myexplode, shadow = True)\n",
    "# plt.legend(title='Shakir',loc =\"upper left\",)\n",
    "# plt.show() \n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.pie(data1, labels = labels, explode = myexplode, shadow = True)\n",
    "# plt.subplot(133)\n",
    "# plt.pie(data2, labels = labels, explode = myexplode, shadow = True)\n",
    "# plt.show()\n",
    "\n",
    "# create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.pie(data1, # Values\n",
    "        labels = labels, # Labels for each sections\n",
    "        explode = myexplode, # To slice the perticuler section\n",
    "        autopct = \"%0.2f%%\", # Show data in persentage for with 2 decimal point\n",
    "        shadow = True, # Showing shadow of pie chart\n",
    "        radius = 1.2, # Radius to increase or decrease the size of pie chart \n",
    "        startangle = 270, # Start angle of first section\n",
    "        )\n",
    "ax2.pie(data2, # Values\n",
    "        labels = labels, # Labels for each sections\n",
    "        explode = myexplode, # To slice the perticuler section\n",
    "        autopct = \"%0.2f%%\", # Show data in persentage for with 2 decimal point\n",
    "        shadow = True, # Showing shadow of pie chart\n",
    "        radius = 1.2, # Radius to increase or decrease the size of pie chart \n",
    "        startangle = 270, # Start angle of first section\n",
    "        )\n",
    "# plot each pie chart in a separate subplot\n",
    "#ax1.pie(data1, labels = labels, explode = myexplode, shadow = True)\n",
    "#ax2.pie(data2, labels = labels, explode = myexplode, shadow = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd76af128d7c40b48583a811a9c1879190eadb08cc3a308c5c2f1ddbc04e5a8b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
